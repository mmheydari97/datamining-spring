{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "colab_type": "code",
    "id": "OTuxi-1k1F2C",
    "outputId": "14134717-8a5a-4253-f0df-2eacc390e583"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "W15eefa2Q5hA",
    "outputId": "33bf8b1a-bb2e-4ff0-f962-97a7876dee58"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoDjozMFREDU"
   },
   "source": [
    "# TensorFlow 2.0 + Keras Overview for Deep Learning Researchers\n",
    "\n",
    "*@fchollet, October 2019*\n",
    "\n",
    "---\n",
    "\n",
    "**This document serves as an introduction, crash course, and quick API reference for TensorFlow 2.0.**\n",
    "\n",
    "---\n",
    "\n",
    "TensorFlow and Keras were both released over four years ago (March 2015 for Keras and November 2015 for TensorFlow). That's a long time in deep learning years!\n",
    "\n",
    "In the old days, TensorFlow 1.x + Keras had a number of known issues:\n",
    "- Using TensorFlow meant manipulating static computation graphs, which would feel awkward and difficult to programmers used to imperative styles of coding.\n",
    "- While the TensorFlow API was very powerful and flexible, it lacked polish and was often confusing or difficult to use.\n",
    "- While Keras was very productive and easy to use, it would often lack flexibility for research use cases.\n",
    "\n",
    "---\n",
    "### TensorFlow 2.0 is an extensive redesign of TensorFlow and Keras that takes into account over four years of user feedback and technical progress. It fixes the issues above in a big way.\n",
    "\n",
    "### It's a machine learning platform from the future.\n",
    "---\n",
    "\n",
    "TensorFlow 2.0 is built on the following key ideas:\n",
    "\n",
    "- Let users run their computation eagerly, like they would in Numpy. This makes TensorFlow 2.0 programming intuitive and Pythonic.\n",
    "- Preserve the considerable advantages of compiled graphs (for performance, distribution, and deployment). This makes TensorFlow fast, scalable, and production-ready.\n",
    "- Leverage Keras as its high-level deep learning API, making TensorFlow approachable and highly productive.\n",
    "- Extend Keras into a spectrum of workflows ranging from the very high-level (easier to use, less flexible) to the very low-level (requires more expertise, but provides great flexibility).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U71NYDeFkUhq"
   },
   "source": [
    "# Part 1: TensorFlow basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2e8-qrcl2kH"
   },
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PX6JvH4h0zyY"
   },
   "source": [
    "This is a [constant](https://www.tensorflow.org/api_docs/python/tf/constant) tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "pGB6GDsfRFJs",
    "outputId": "1ecfd444-c24d-4f53-9df3-2e9bdd6fa04e"
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RX2SB_2O1jx7"
   },
   "source": [
    "You can get its value as a Numpy array by calling `.numpy()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "EwGyHOoq1oWn",
    "outputId": "069f2ad4-f303-4d13-9c30-ba84d9b83138"
   },
   "outputs": [],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNkno66r1xvg"
   },
   "source": [
    "Much like a Numpy array, it features the attributes `dtype` and `shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "tSxtblSP13v2",
    "outputId": "6d593176-e23f-4076-f462-ab3d60c43a90"
   },
   "outputs": [],
   "source": [
    "print('dtype:', x.dtype)\n",
    "print('shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oogzv3--2EF2"
   },
   "source": [
    "A common way to create constant tensors is via `tf.ones` and `tf.zeros` (just like `np.ones` and `np.zeros`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "9qDlfa8r2Lia",
    "outputId": "858715cf-eff5-4697-bc2a-0a2596e16eb2"
   },
   "outputs": [],
   "source": [
    "print(tf.ones(shape=(2, 1)))\n",
    "print(tf.zeros(shape=(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qzGYEkdcmYbe"
   },
   "source": [
    "## Random constant tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk94gREJ2r-e"
   },
   "source": [
    "This is all pretty [normal](https://www.tensorflow.org/api_docs/python/tf/random/normal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "jqRrO-Puma7-",
    "outputId": "f8497401-8d70-4c08-877a-3921e2810097"
   },
   "outputs": [],
   "source": [
    "tf.random.normal(shape=(2, 2), mean=0., stddev=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wL0EMPT93SEU"
   },
   "source": [
    "And here's an integer tensor with values drawn from a random [uniform](https://www.tensorflow.org/api_docs/python/tf/random/uniform) distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "9syARhtj2wbx",
    "outputId": "f6fb950a-13c2-4189-b1fd-7a7e0bd69c66"
   },
   "outputs": [],
   "source": [
    "tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I95066exmbDU"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMflzgPM3Mim"
   },
   "source": [
    "[Variables](https://www.tensorflow.org/guide/variable) are special tensors used to store mutable state (like the weights of a neural network). You create a Variable using some initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "8FvENXmBmcyT",
    "outputId": "696552d6-dc04-4762-b4e8-32dac631ee3c"
   },
   "outputs": [],
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(initial_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRFwVySi3biu"
   },
   "source": [
    "You update the value of a Variable by using the methods `.assign(value)`, or `.assign_add(increment)` or `.assign_sub(decrement)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EOCsCNvc3mNR"
   },
   "outputs": [],
   "source": [
    "new_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign(new_value)\n",
    "for i in range(2):\n",
    "  for j in range(2):\n",
    "    assert a[i, j] == new_value[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrSjwl_056j8"
   },
   "outputs": [],
   "source": [
    "added_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign_add(added_value)\n",
    "for i in range(2):\n",
    "  for j in range(2):\n",
    "    assert a[i, j] == new_value[i, j] + added_value[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAIqYQmOl_wR"
   },
   "source": [
    "## Doing math in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bmtTepn6SvG"
   },
   "source": [
    "You can use TensorFlow exactly like you would use Numpy. The main difference is that your TensorFlow code can run on GPU and TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCZGHQ_XmHuZ"
   },
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "c = a + b\n",
    "d = tf.square(c)\n",
    "e = tf.exp(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Feq3qWoBVQW"
   },
   "source": [
    "## Computing gradients with `GradientTape`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdsmOcrJBWXe"
   },
   "source": [
    "Oh, and there's another big difference with Numpy: you can automatically retrieve the gradient of any differentiable expression.\n",
    "\n",
    "Just open a [`GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape), start \"watching\" a tensor via `tape.watch()`, and compose a differentiable expression using this tensor as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "FkEAY45IBjPv",
    "outputId": "d54cfeeb-6a83-4ea5-829d-5deaf22e2d28"
   },
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(a)  # Start recording the history of operations applied to `a`\n",
    "  c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`\n",
    "  # What's the gradient of `c` with respect to `a`?\n",
    "  dc_da = tape.gradient(c, a)\n",
    "  print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8UpqFx_DDbV"
   },
   "source": [
    "By default, variables are watched automatically, so you don't need to manually `watch` them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "OtH3FuvDDOAY",
    "outputId": "93fd37ce-917d-4e25-9499-227850621d19"
   },
   "outputs": [],
   "source": [
    "a = tf.Variable(a)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "  dc_da = tape.gradient(c, a)\n",
    "  print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFlBGjuEDbt-"
   },
   "source": [
    "Note that you can compute higher-order derivatives by nesting tapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "cjPcY0OIDhEA",
    "outputId": "b61cac86-faf7-4296-9486-716d3eb1e9df"
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as outer_tape:\n",
    "  with tf.GradientTape() as tape:\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c, a)\n",
    "  d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "  print(d2c_da2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KC5RgwGeBP-9"
   },
   "source": [
    "## An end-to-end example: linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Owbx4mlEErNN"
   },
   "source": [
    "So far you've learned that TensorFlow is a Numpy-like library that is GPU or TPU accelerated, with automatic differentiation. Time for an end-to-end example: let's implement a linear regression, the FizzBuzz of Machine Learning. \n",
    "\n",
    "For the sake of demonstration, we won't use any of the higher-level Keras components like `Layer` or `MeanSquaredError`. Just basic ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uhitqoj2FH8U"
   },
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# This is our weight matrix\n",
    "w = tf.Variable(tf.random.uniform(shape=(input_dim, output_dim)))\n",
    "# This is our bias vector\n",
    "b = tf.Variable(tf.zeros(shape=(output_dim,)))\n",
    "\n",
    "def compute_predictions(features):\n",
    "  return tf.matmul(features, w) + b\n",
    "\n",
    "def compute_loss(labels, predictions):\n",
    "  return tf.reduce_mean(tf.square(labels - predictions))\n",
    "\n",
    "def train_on_batch(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = compute_predictions(x)\n",
    "    loss = compute_loss(y, predictions)\n",
    "    dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
    "  w.assign_sub(learning_rate * dloss_dw)\n",
    "  b.assign_sub(learning_rate * dloss_db)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qC1fp3BYJeXo"
   },
   "source": [
    "Let's generate some artificial data to demonstrate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "6ocAkrliMVAQ",
    "outputId": "7d3407c8-3b2a-442e-d82d-da1391618aa5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Prepare a dataset.\n",
    "num_samples = 10000\n",
    "negative_samples = np.random.multivariate_normal(\n",
    "    mean=[0, 3], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\n",
    "positive_samples = np.random.multivariate_normal(\n",
    "    mean=[3, 0], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\n",
    "features = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n",
    "labels = np.vstack((np.zeros((num_samples, 1), dtype='float32'),\n",
    "                    np.ones((num_samples, 1), dtype='float32')))\n",
    "\n",
    "plt.scatter(features[:, 0], features[:, 1], c=labels[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCdZTpjlJlGo"
   },
   "source": [
    "Now let's train our linear regression by iterating over batch-by-batch over the data and repeatedly calling `train_on_batch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "YsHszjjaJDQZ",
    "outputId": "3bcdff4e-5fc3-42ae-90fa-1189e5abcd77"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data.\n",
    "random.Random(1337).shuffle(features)\n",
    "random.Random(1337).shuffle(labels)\n",
    "\n",
    "# Create a tf.data.Dataset object for easy batched iteration\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(256)\n",
    "\n",
    "for epoch in range(10):\n",
    "  for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, y)\n",
    "  print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIDDhTcyJwSM"
   },
   "source": [
    "Here's how our model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "oBPYQpskJxxT",
    "outputId": "033450dd-6a62-409d-9f20-c8722257a259"
   },
   "outputs": [],
   "source": [
    "predictions = compute_predictions(features)\n",
    "plt.scatter(features[:, 0], features[:, 1], c=predictions[:, 0] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBcqiop7mH7x"
   },
   "source": [
    "## Making it fast with `tf.function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjZ8kuruNdj6"
   },
   "source": [
    "But how fast is our current code running?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NXoe7S5RmStB",
    "outputId": "ca0e9151-e9e5-4406-f328-fcbdbc6d8c1f"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(20):\n",
    "  for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, y)\n",
    "t_end = time.time() - t0\n",
    "print('Time per epoch: %.3f s' % (t_end / 20,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHsvPqyRN_E3"
   },
   "source": [
    "Let's compile the training function into a static graph. Literally all we need to do is add the `tf.function` decorator on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEYFkThcOGcg"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_on_batch(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = compute_predictions(x)\n",
    "    loss = compute_loss(y, predictions)\n",
    "    dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
    "  w.assign_sub(learning_rate * dloss_dw)\n",
    "  b.assign_sub(learning_rate * dloss_db)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocOuskvoOKsx"
   },
   "source": [
    "Let's try this again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KT2w6DVmONB5",
    "outputId": "bb51ad94-123c-49c1-f24f-3cca7eada71a"
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for epoch in range(20):\n",
    "  for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, y)\n",
    "t_end = time.time() - t0\n",
    "print('Time per epoch: %.3f s' % (t_end / 20,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYPWZaSqOfEL"
   },
   "source": [
    "40% reduction, neat. In this case we used a trivially simple model; in general the bigger the model the greater the speedup you can get by leveraging static graphs.\n",
    "\n",
    "Remember: eager execution is great for debugging and printing results line-by-line, but when it's time to scale, static graphs are a researcher's best friends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3I3v_FqjFty"
   },
   "source": [
    "# Part 2: The Keras API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjLI719fPfJi"
   },
   "source": [
    "Keras is a Python API for deep learning. It has something for everyone:\n",
    "\n",
    "- If you're an engineer, Keras provides you with reusable blocks such as layers, metrics, training loops, to support common use cases. It provides a high-level user experience that's accessible and productive.\n",
    "\n",
    "- If you're a researcher, you may prefer not to use these built-in blocks such as layers and training loops, and instead create your own. Of course, Keras allows you to do this. In this case, Keras provides you with templates for the blocks you write, it provides you with structure, with an API standard for things like Layers and Metrics. This structure makes your code easy to share with others and easy to integrate in production workflows.\n",
    "\n",
    "- The same is true for library developers: TensorFlow is a large ecosystem. It has many different libraries. In order for different libraries to be able to talk to each other and share components, they need to follow an API standard. That's what Keras provides.\n",
    "\n",
    "Crucially, Keras brings high-level UX and low-level flexibility together fluently: you no longer have on one hand, a high-level API that's easy to use but inflexible, and on the other hand a low-level API that's flexible but only approachable by experts. Instead, you have a spectrum of workflows, from the  very high-level to the very low-level. Workflows that are all compatible because they're built on top of the same concepts and objects.\n",
    "\n",
    "![Spectrum of Keras workflows](https://keras-dev.s3.amazonaws.com/tutorials-img/spectrum-of-workflows.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9DSVjdHPkOw"
   },
   "source": [
    "## The base `Layer` class\n",
    "\n",
    "The first class you need to know is [`Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer). Pretty much everything in Keras derives from it.\n",
    "\n",
    "A Layer encapsulates a state (weights) and some computation (defined in the `call` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Io3dUQzaPnPc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Linear(Layer):\n",
    "  \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "  def __init__(self, units=32, input_dim=32):\n",
    "      super(Linear, self).__init__()\n",
    "      w_init = tf.random_normal_initializer()\n",
    "      self.w = tf.Variable(\n",
    "          initial_value=w_init(shape=(input_dim, units), dtype='float32'),\n",
    "          trainable=True)\n",
    "      b_init = tf.zeros_initializer()\n",
    "      self.b = tf.Variable(\n",
    "          initial_value=b_init(shape=(units,), dtype='float32'),\n",
    "          trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "      return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "# Instantiate our layer.\n",
    "linear_layer = Linear(4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vo3etyK8BO4a"
   },
   "source": [
    "A layer instance works like a function. Let's call it on some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBUCLfHVBQLF"
   },
   "outputs": [],
   "source": [
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert y.shape == (2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXqpznsxBaCC"
   },
   "source": [
    "The `Layer` class takes care of tracking the weights assigned to it as attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_FaUtEYBeJw"
   },
   "outputs": [],
   "source": [
    "# Weights are automatically tracked under the `weights` property.\n",
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6PZ6QXUHdxA"
   },
   "source": [
    "Note that's also a shortcut method for creating weights: `add_weight`. Instead of doing\n",
    "\n",
    "```python\n",
    "w_init = tf.random_normal_initializer()\n",
    "self.w = tf.Variable(initial_value=w_init(shape=shape, dtype='float32'))\n",
    "```\n",
    "\n",
    "You would typically do:\n",
    "\n",
    "```python\n",
    "self.w = self.add_weight(shape=shape, initializer='random_normal')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lphpMGIiHRUP"
   },
   "source": [
    "It’s good practice to create weights in a separate `build` method, called lazily with the shape of the first inputs seen by your layer. Here, this pattern prevents us from having to specify input_dim in the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpPjScZKHXhS"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "  \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "  def __init__(self, units=32):\n",
    "      super(Linear, self).__init__()\n",
    "      self.units = units\n",
    "\n",
    "  def build(self, input_shape):\n",
    "      self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "      self.b = self.add_weight(shape=(self.units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "      return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "# Instantiate our lazy layer.\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert len(linear_layer.weights) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86khdsF3Pnr0"
   },
   "source": [
    "## Trainable and non-trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32zvCEKLICr5"
   },
   "source": [
    "Weights created by layers can be either trainable or non-trainable. They're exposed in `trainable_weights` and `non_trainable_weights`. Here's a layer with a non-trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "LZ8s28NnX20u",
    "outputId": "4080f7eb-9217-42a4-c9d9-95da09ee6eb6"
   },
   "outputs": [],
   "source": [
    "class ComputeSum(Layer):\n",
    "  \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "  def __init__(self, input_dim):\n",
    "      super(ComputeSum, self).__init__()\n",
    "      # Create a non-trainable weight.\n",
    "      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
    "                               trainable=False)\n",
    "\n",
    "  def call(self, inputs):\n",
    "      self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "      return self.total  \n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [2. 2.]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [4. 4.]\n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oBkX6ZfYO8j"
   },
   "source": [
    "## Recursively composing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YeLDL9MJI2dK"
   },
   "source": [
    "Layers can be recursively nested to create bigger computation blocks. Each layer will track the weights of its sublayers (both trainable and non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5HBH-dtYQuk"
   },
   "outputs": [],
   "source": [
    "# Let's reuse the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "\n",
    "class MLP(Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# The first call to the `mlp` object will create the weights.\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked.\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WavMVtXGQk-z"
   },
   "source": [
    "## Built-in layers\n",
    "\n",
    "Keras provides you with a [wide range of built-in layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/), so that you don't have to implement your own layers all the time.\n",
    "\n",
    "- Convolution layers\n",
    "- Transposed convolutions\n",
    "- Separateable convolutions\n",
    "- Average and max pooling\n",
    "- Global average and max pooling\n",
    "- LSTM, GRU (with built-in cuDNN acceleration)\n",
    "- BatchNormalization\n",
    "- Dropout\n",
    "- Attention\n",
    "- ConvLSTM2D\n",
    "- etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdrw7OppQ6At"
   },
   "source": [
    "Keras follows the principles of exposing good default configurations, so that layers will work fine out of the box for most use cases if you leave keyword arguments to their default value. For instance, the `LSTM` layer uses an orthogonal recurrent matrix initializer by default, and initializes the forget gate bias to one by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oq88tadFz8Z"
   },
   "source": [
    "## The `training` argument in `call`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A2NkTT0AQV8j"
   },
   "source": [
    "Some layers, in particular the `BatchNormalization` layer and the `Dropout` layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a `training` (boolean) argument in the `call` method.\n",
    "\n",
    "By exposing this argument in `call`, you enable the built-in training and evaluation loops (e.g. `fit`) to correctly use the layer in training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysXzHB5KJiLt"
   },
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "  \n",
    "  def __init__(self, rate):\n",
    "    super(Dropout, self).__init__()\n",
    "    self.rate = rate\n",
    "\n",
    "  def call(self, inputs, training=None):\n",
    "    if training:\n",
    "      return tf.nn.dropout(inputs, rate=self.rate)\n",
    "    return inputs\n",
    "\n",
    "class MLPWithDropout(Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "      super(MLPWithDropout, self).__init__()\n",
    "      self.linear_1 = Linear(32)\n",
    "      self.dropout = Dropout(0.5)\n",
    "      self.linear_3 = Linear(10)\n",
    "\n",
    "  def call(self, inputs, training=None):\n",
    "      x = self.linear_1(inputs)\n",
    "      x = tf.nn.relu(x)\n",
    "      x = self.dropout(x, training=training)\n",
    "      return self.linear_3(x)\n",
    "    \n",
    "mlp = MLPWithDropout()\n",
    "y_train = mlp(tf.ones((2, 2)), training=True)\n",
    "y_test = mlp(tf.ones((2, 2)), training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyC7KfV-YcYS"
   },
   "source": [
    "## A more Functional way of defining models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxbEQANKQB6F"
   },
   "source": [
    "To build deep learning models, you don't have to use object-oriented programming all the time. Layers can also be composed functionally, like this (we call it the \"Functional API\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jiL-0N7sYc6X"
   },
   "outputs": [],
   "source": [
    "# We use an `Input` object to describe the shape and dtype of the inputs.\n",
    "# This is the deep learning equivalent of *declaring a type*.\n",
    "# The shape argument is per-sample; it does not include the batch size.\n",
    "# The functional API focused on defining per-sample transformations.\n",
    "# The model we create will automatically batch the per-sample transformations,\n",
    "# so that it can be called on batches of data.\n",
    "inputs = tf.keras.Input(shape=(16,))\n",
    "\n",
    "# We call layers on these \"type\" objects\n",
    "# and they return updated types (new shapes/dtypes).\n",
    "x = Linear(32)(inputs) # We are reusing the Linear layer we defined earlier.\n",
    "x = Dropout(0.5)(x) # We are reusing the Dropout layer we defined earlier.\n",
    "outputs = Linear(10)(x)\n",
    "\n",
    "# A functional `Model` can be defined by specifying inputs and outputs.\n",
    "# A model is itself a layer like any other.\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# A functional model already has weights, before being called on any data.\n",
    "# That's because we defined its input shape in advance (in `Input`).\n",
    "assert len(model.weights) == 4\n",
    "\n",
    "# Let's call our model on some data.\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vK5HqnT3Xgcz"
   },
   "source": [
    "The Functional API tends to be more concise than subclassing, and provides a few other advantages (generally the same advantages that functional, typed languages provide over untyped OO development). However, it can only be used to define DAGs of layers -- recursive networks should be defined as `Layer` subclasses instead.\n",
    "\n",
    "Key differences between models defined via subclassing and Functional models are explained in [this blog post](https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021).\n",
    "\n",
    "Learn more about the Functional API [here](https://www.tensorflow.org/alpha/guide/keras/functional).\n",
    "\n",
    "In your research workflows, you may often find yourself mix-and-matching OO models and Functional models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p0KngmPTScu"
   },
   "source": [
    "For models that are simple stacks of layers with a single input and a single output, you can also use the `Sequential` class which turns a list of layers into a `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNhTY6frTaP2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential([Linear(32), Dropout(0.5), Linear(10)])\n",
    "\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cydf3i_FFXlh"
   },
   "source": [
    "## Loss classes\n",
    "\n",
    "Keras features a wide range of built-in loss classes, like `BinaryCrossentropy`, `CategoricalCrossentropy`, `KLDivergence`, etc. They work like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "019Nm1eWFaUO",
    "outputId": "b8a9dc36-93f3-4f46-8953-92926e05a805"
   },
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "y_true = [0., 0., 1., 1.]  # Targets\n",
    "y_pred = [1., 1., 1., 0.]  # Predictions\n",
    "loss = bce(y_true, y_pred)\n",
    "print('Loss:', loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smbxFMGXY83U"
   },
   "source": [
    "Note that loss classes are stateless: the output of `__call__` is only a function of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNLZsnswFbE_"
   },
   "source": [
    "## Metric classes\n",
    "\n",
    "Keras also features a wide range of built-in metric classes, such as `BinaryAccuracy`, `AUC`, `FalsePositives`, etc.\n",
    "\n",
    "Unlike losses, metrics are stateful. You update their state using the `update_state` method, and you query the scalar metric result using `result`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "4dUZkMWATKMC",
    "outputId": "f9179b23-dfac-4b70-a26f-cd01841e794f"
   },
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.AUC()\n",
    "m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n",
    "print('Intermediate result: ', m.result().numpy())\n",
    "\n",
    "m.update_state([1, 1, 1, 1], [0, 1, 1, 0])\n",
    "print('Final result: ', m.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "doUSrciie2Px"
   },
   "source": [
    "The internal state can be cleared with `metric.reset_states`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwx7DjFBZ-C-"
   },
   "source": [
    "You can easily roll out your own metrics by subclassing the `Metric` class:\n",
    "\n",
    "- Create the state variables in `__init__`\n",
    "- Update the variables given `y_true` and `y_pred` in `update_state`\n",
    "- Return the metric result in `result`\n",
    "- Clear the state in `reset_states`\n",
    "\n",
    "Here's a quick implementation of a `BinaryTruePositives` metric as a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vVByLrJyaBx_"
   },
   "outputs": [],
   "source": [
    "class BinaryTruePositives(tf.keras.metrics.Metric):\n",
    "\n",
    "  def __init__(self, name='binary_true_positives', **kwargs):\n",
    "    super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n",
    "    self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    y_true = tf.cast(y_true, tf.bool)\n",
    "    y_pred = tf.cast(y_pred, tf.bool)\n",
    "\n",
    "    values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
    "    values = tf.cast(values, self.dtype)\n",
    "    if sample_weight is not None:\n",
    "      sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "      sample_weight = tf.broadcast_weights(sample_weight, values)\n",
    "      values = tf.multiply(values, sample_weight)\n",
    "    self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "  def result(self):\n",
    "    return self.true_positives\n",
    "\n",
    "  def reset_states(self):\n",
    "    self.true_positive.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0PdvHdAdQl0"
   },
   "source": [
    "## Optimizer classes & a quick end-to-end training loop\n",
    "\n",
    "You don't normally have to define by hand how to update your variables during gradient descent, like we did in our initial linear regression example. You would usually use one of the built-in Keras optimizer, like `SGD`, `RMSprop`, or `Adam`.\n",
    "\n",
    "Here's a simple MNSIT example that brings together loss classes, metric classes, and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "4jNl1ykEdkj8",
    "outputId": "d9c45155-d2b9-48cd-abff-24d29cdaaafa"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train[:].reshape(60000, 784).astype('float32') / 255\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Instantiate a simple classification model\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(256, activation=tf.nn.relu),\n",
    "  layers.Dense(256, activation=tf.nn.relu),\n",
    "  layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an accuracy metric.\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Iterate over the batches of the dataset.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "  \n",
    "  # Open a GradientTape.\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Forward pass.\n",
    "    logits = model(x)\n",
    "\n",
    "    # Loss value for this batch.\n",
    "    loss_value = loss(y, logits)\n",
    "     \n",
    "  # Get gradients of weights wrt the loss.\n",
    "  gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "  \n",
    "  # Update the weights of our linear layer.\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "  # Update the running accuracy.\n",
    "  accuracy.update_state(y, logits)\n",
    "  \n",
    "  # Logging.\n",
    "  if step % 100 == 0:\n",
    "    print('Step:', step)\n",
    "    print('Loss from last step:', float(loss_value))\n",
    "    print('Total running accuracy so far:', float(accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIJYBrXoekXD"
   },
   "source": [
    "We can reuse our `SparseCategoricalAccuracy` metric instance to implement a testing loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sl6FKvqbeqX9",
    "outputId": "2a1c83a3-8c0a-4a1c-8262-0625a189f533"
   },
   "outputs": [],
   "source": [
    "x_test = x_test[:].reshape(10000, 784).astype('float32') / 255\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(128)\n",
    "\n",
    "accuracy.reset_states()  # This clears the internal state of the metric\n",
    "\n",
    "for step, (x, y) in enumerate(test_dataset):\n",
    "  logits = model(x)\n",
    "  accuracy.update_state(y, logits)\n",
    "\n",
    "print('Final test accuracy:', float(accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CEP7jzC8YVWy"
   },
   "source": [
    "## The `add_loss` method\n",
    "\n",
    "Sometimes you need to compute loss values on the fly during a foward pass (especially regularization losses). Keras allows you to compute loss values at any time, and to recursively keep track of them via the `add_loss` method.\n",
    "\n",
    "Here's an example of a layer that adds a sparsity regularization loss based on the L2 norm of the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbBVP--jYgHg"
   },
   "outputs": [],
   "source": [
    "class ActivityRegularization(Layer):\n",
    "  \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
    "  \n",
    "  def __init__(self, rate=1e-2):\n",
    "    super(ActivityRegularization, self).__init__()\n",
    "    self.rate = rate\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    # We use `add_loss` to create a regularization loss\n",
    "    # that depends on the inputs.\n",
    "    self.add_loss(self.rate * tf.reduce_sum(tf.square(inputs)))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4qoQk7abK5v"
   },
   "source": [
    "Loss values added via `add_loss` can be retrieved in the `.losses` list property of any `Layer` or `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VlJc_4pbbQ2N",
    "outputId": "5d7d6881-12e4-4e96-8a03-10b4b5586164"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class SparseMLP(Layer):\n",
    "  \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n",
    "\n",
    "  def __init__(self, output_dim):\n",
    "      super(SparseMLP, self).__init__()\n",
    "      self.dense_1 = layers.Dense(32, activation=tf.nn.relu)\n",
    "      self.regularization = ActivityRegularization(1e-2)\n",
    "      self.dense_2 = layers.Dense(output_dim)\n",
    "\n",
    "  def call(self, inputs):\n",
    "      x = self.dense_1(inputs)\n",
    "      x = self.regularization(x)\n",
    "      return self.dense_2(x)\n",
    "    \n",
    "\n",
    "mlp = SparseMLP(1)\n",
    "y = mlp(tf.ones((10, 10)))\n",
    "\n",
    "print(mlp.losses)  # List containing one float32 scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkI3GA2TbWvY"
   },
   "source": [
    "These losses are cleared by the top-level layer at the start of each forward pass -- they don't accumulate. So `layer.losses` always contain only the losses created during the last forward pass. You would typically use these losses by summing them before computing your gradients when writing a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "2m0xNYGEbZe2",
    "outputId": "f08b32b7-9893-48d0-e137-ea4ab31d82d7"
   },
   "outputs": [],
   "source": [
    "# Losses correspond to the *last* forward pass.\n",
    "mlp = SparseMLP(1)\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1  # No accumulation.\n",
    "\n",
    "# Let's demonstrate how to use these losses in a training loop.\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# A new MLP.\n",
    "mlp = SparseMLP(10)\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Forward pass.\n",
    "    logits = mlp(x)\n",
    "\n",
    "    # External loss value for this batch.\n",
    "    loss = loss_fn(y, logits)\n",
    "    \n",
    "    # Add the losses created during the forward pass.\n",
    "    loss += sum(mlp.losses)\n",
    "     \n",
    "    # Get gradients of weights wrt the loss.\n",
    "    gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "  \n",
    "  # Update the weights of our linear layer.\n",
    "  optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "  \n",
    "  # Logging.\n",
    "  if step % 100 == 0:\n",
    "    print(step, float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zecO65f2Ph7O"
   },
   "source": [
    "## A detailed end-to-end example: a Variational AutoEncoder (VAE)\n",
    "\n",
    "If you want to take a break from the basics and look at a slightly more advanced example, check out this [Variational AutoEncoder](https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example) implementation that demonstrates everything you've learned so far:\n",
    "\n",
    "- Subclassing `Layer`\n",
    "- Recursive layer composition\n",
    "- Loss classes and metric classes\n",
    "- `add_loss`\n",
    "- `GradientTape`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7V0yRrfYFuVT"
   },
   "source": [
    "## Using built-in training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNwAjgEXPpnP"
   },
   "source": [
    "It would be a bit silly if you had to write your own low-level training loops every time for simple use cases. Keras provides you with a built-in training loop on the `Model` class. If you want to use it, either subclass from `Model` or create a `Functional` or `Sequential` model.\n",
    "\n",
    "To demonstrate it, let's reuse the MNIST setup from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omdNf2x4jovv"
   },
   "outputs": [],
   "source": [
    "# Prepare a dataset.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Instantiate a simple classification model\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(256, activation=tf.nn.relu),\n",
    "  layers.Dense(256, activation=tf.nn.relu),\n",
    "  layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an accuracy metric.\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYU5PkaLiriO"
   },
   "source": [
    "First, call `compile` to configure the optimizer, loss, and metrics to monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eo6GhvzjJjdb"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98zDjMPej06U"
   },
   "source": [
    "Then we call `fit` on our model to pass it the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "-OdztL4nj4ed",
    "outputId": "f5c93e3e-c55e-4f83-dc96-84fe9776688a"
   },
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZc2ss8qln1p"
   },
   "source": [
    "Done! Now let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "d0-XQbxOlqB3",
    "outputId": "42f396a4-0e45-4fb7-c2ba-94ed15f2ecbe"
   },
   "outputs": [],
   "source": [
    "x_test = x_test[:].reshape(10000, 784).astype('float32') / 255\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(128)\n",
    "\n",
    "loss, acc = model.evaluate(test_dataset)\n",
    "print('loss:', loss, 'acc:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkmbEn55nV7y"
   },
   "source": [
    "Note that you can also monitor your loss and metrics on some validation data during `fit`.\n",
    "\n",
    "Also, you can call `fit` directly on Numpy arrays, so no need for the dataset conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "7q8UuNivngVe",
    "outputId": "650ef673-5bf7-47e0-a3a8-fac61bba9edd"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "\n",
    "num_val_samples = 10000\n",
    "x_val = x_train[-num_val_samples:]\n",
    "y_val = y_train[-num_val_samples:]\n",
    "x_train = x_train[:-num_val_samples]\n",
    "y_train = y_train[:-num_val_samples]\n",
    "\n",
    "# Instantiate a simple classification model\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(256, activation=tf.nn.relu),\n",
    "  layers.Dense(256, activation=tf.nn.relu),\n",
    "  layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an accuracy metric.\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=[accuracy])\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=3,\n",
    "          batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88ExjKfCo7aP"
   },
   "source": [
    "## Callbacks\n",
    "\n",
    "One of the neat features of `fit` (besides built-in support for sample weighting and class weighting) is that you can easily customize what happens during training and evaluation by using [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/).\n",
    "\n",
    "A callback is an object that is called at different points during training (e.g. at the end of every batch or at the end of every epoch) and does stuff.\n",
    "\n",
    "There's a bunch of built-in callback available, like `ModelCheckpoint` to save your models after each epoch during training, or `EarlyStopping`, which interrupts training when your validation metrics start stalling.\n",
    "\n",
    "And you can easily [write your own callbacks](https://www.tensorflow.org/guide/keras/custom_callback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "id": "oAylVdYJqcZ3",
    "outputId": "a1eae962-db10-4a7a-f1e0-524b51f11272"
   },
   "outputs": [],
   "source": [
    "# Instantiate a simple classification model\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(256, activation=tf.nn.relu),\n",
    "  layers.Dense(256, activation=tf.nn.relu),\n",
    "  layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an accuracy metric.\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=[accuracy])\n",
    "\n",
    "# Instantiate some callbacks\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath='my_model.keras',\n",
    "                                                save_best_only=True)]\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=30,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxINLLGitX_n"
   },
   "source": [
    "# Parting words\n",
    "\n",
    "I hope this guide has given you a good overview of what's possible with TensorFlow 2.0 and Keras!\n",
    "\n",
    "Remember that TensorFlow and Keras don't represent a single workflow. It's a spectrum of workflows, each with its own trade-off between usability and flexibility. For instance, you've noticed that it's much easier to use `fit` than to write a custom training loop, but `fit` doesn't give you the same level of granular control for research use cases.\n",
    "\n",
    "So use the right tool for the job!\n",
    "\n",
    "A core principle of Keras is \"progressive disclosure of complexity\": it's easy to get started, and you can gradually dive into workflows where you write more and more logic from scratch, providing you with complete control.\n",
    "\n",
    "This applies to both model definition, and model training.\n",
    "\n",
    "![Model definition: spectrum of workflows](https://keras-dev.s3.amazonaws.com/tutorials-img/model-building-spectrum.png)\n",
    "\n",
    "![Model training: spectrum of workflows](https://keras-dev.s3.amazonaws.com/tutorials-img/model-training-spectrum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfO_uy61upRm"
   },
   "source": [
    "\n",
    "## What to learn next\n",
    "\n",
    "Next, there are many more topics you may be interested in:\n",
    "\n",
    "- [Saving and serialization](https://www.tensorflow.org/guide/keras/save_and_serialize)\n",
    "- [Distributed training on multiple GPUS](https://www.tensorflow.org/guide/distributed_training)\n",
    "- [Exporting models to TFLite for deployment on Android or embedded systems](https://www.tensorflow.org/lite/convert/python_api#converting_a_keras_model_)\n",
    "- [Exporting models to TensorFlow.js for deployment in the browser](https://www.tensorflow.org/js/tutorials/conversion/import_keras)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlow 2.0 + Keras Crash Course.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
