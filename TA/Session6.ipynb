{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Naive Bayes and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(columns=['words', 'sentiment'])\n",
    "df_test = pd.DataFrame(columns=['words', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_dir = './Data/6/sw.txt'\n",
    "stop_words = []\n",
    "with open(sw_dir) as f:\n",
    "    text = f.readlines()\n",
    "    for word in text:\n",
    "        stop_words.append(re.findall('\\S+', word)[0])\n",
    "\n",
    "# adding br and empty string to stop words\n",
    "stop_words.append('br')\n",
    "stop_words.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from text files\n",
    "train_pos_dir = './Data/6/aclImdb/train/pos'\n",
    "for filename in os.listdir(train_pos_dir):\n",
    "    with open(os.path.join(train_pos_dir, filename)) as f:\n",
    "        text = f.readlines()[0]\n",
    "        df_train = df_train.append({'words': text, 'sentiment': 1},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "train_neg_dir = './Data/6/aclImdb/train/neg'\n",
    "for filename in os.listdir(train_neg_dir):\n",
    "    with open(os.path.join(train_neg_dir, filename)) as f:\n",
    "        text = f.readlines()[0]\n",
    "        df_train = df_train.append({'words': text, 'sentiment': 0},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "test_pos_dir = './Data/6/aclImdb/test/pos'\n",
    "for filename in os.listdir(test_pos_dir):\n",
    "    with open(os.path.join(test_pos_dir, filename)) as f:\n",
    "        text = f.readlines()[0]\n",
    "        df_test = df_test.append({'words': text, 'sentiment': 1},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "test_neg_dir = './Data/6/aclImdb/test/neg'\n",
    "for filename in os.listdir(test_neg_dir):\n",
    "    with open(os.path.join(test_neg_dir, filename)) as f:\n",
    "        text = f.readlines()[0]\n",
    "        df_test = df_test.append({'words': text, 'sentiment': 0},\n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    \n",
    "    def change(ch):\n",
    "        if ch in string.punctuation or ch.isdigit():\n",
    "            return \" \"\n",
    "        else:\n",
    "            return ch\n",
    "    \n",
    "    no_punct = \"\".join([change(ch) for ch in text])\n",
    "    return no_punct\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: remove_punct(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: tokenize(x.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(tokens):\n",
    "    text = [w for w in tokens if w not in stop_words]\n",
    "    return text\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: remove_sw(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short(tokens):\n",
    "    text = [w for w in tokens if len(w)>2]\n",
    "    return text\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: remove_short(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to get root of tokens like stemming and lemmatizing. stemming is faster and lemmatizing is more precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizing(tokens):\n",
    "    text = [wn.lemmatize(w) for w in tokens]\n",
    "    return text\n",
    "\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "def stemming(tokens):\n",
    "    text = [ps.stem(w) for w in tokens]\n",
    "    return text\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: lemmatizing(x))\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_punct(text)\n",
    "    text = tokenize(text)\n",
    "    text = remove_sw(text)\n",
    "    text = remove_short(text)\n",
    "    # text = lemmatizing(text)\n",
    "    text = stemming(text)\n",
    "    return text\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text,\n",
    "                             lowercase=True,\n",
    "                             binary=True)\n",
    "X_train = count_vect.fit_transform(df_train['words'])\n",
    "y_train = df_train['sentiment'].to_numpy(dtype='int')\n",
    "\n",
    "X_test = count_vect.transform(df_test['words'])\n",
    "y_test = df_train['sentiment'].to_numpy(dtype='int')\n",
    "# print(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha=100)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc_tr = clf.score(X_train, y_train)\n",
    "acc_te = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"train accuracy: {}%\".format(acc_tr*100))\n",
    "print(\"test accuracy: {}%\".format(acc_te*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [10**x for x in range(-4, 4)]\n",
    "\n",
    "accs_tr = []\n",
    "accs_te = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    cls = MultinomialNB(alpha=alpha)\n",
    "    cls = cls.fit(X_train, y_train)\n",
    "    accs_tr.append(cls.score(X_train, y_train))\n",
    "    accs_te.append(cls.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(alphas, accs_tr)\n",
    "plt.plot(alphas, accs_te)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train accuracy', 'test accuracy'])\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    \n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (svm.SVC(kernel='linear', C=1.0),\n",
    "          svm.LinearSVC(C=1.0, max_iter=10000),\n",
    "          svm.SVC(kernel='rbf', gamma=0.7, C=1.0),\n",
    "          svm.SVC(kernel='poly', degree=3, gamma='auto', C=1.0))\n",
    "models = (clf.fit(X, y) for clf in models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ('SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel')\n",
    "\n",
    "fig, sub = plt.subplots(2, 2, figsize=(12, 8))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
