{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(columns=['words', 'sentiment'])\n",
    "df_test = pd.DataFrame(columns=['words', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_dir = '../dataset/sw.txt'\n",
    "stop_words = []\n",
    "with open(sw_dir) as f:\n",
    "    text = f.readlines()\n",
    "    for word in text:\n",
    "        stop_words.append(re.findall('\\S+', word)[0])\n",
    "\n",
    "# adding br and empty string to stop words\n",
    "stop_words.append('br')\n",
    "stop_words.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from text files\n",
    "train_pos_dir = '../dataset/train/pos'\n",
    "for filename in os.listdir(train_pos_dir):\n",
    "    with open(os.path.join(train_pos_dir, filename)) as f:\n",
    "        text = f.readlines()[0]\n",
    "        df_train = df_train.append({'words': text, 'sentiment': 1},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "train_neg_dir = '../dataset/train/neg'\n",
    "for filename in os.listdir(train_neg_dir):\n",
    "    with open(os.path.join(train_neg_dir, filename)) as f:\n",
    "        text = f.readlines()[0]\n",
    "        df_train = df_train.append({'words': text, 'sentiment': 0},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "test_pos_dir = '../dataset/test/pos'\n",
    "for filename in os.listdir(test_pos_dir):\n",
    "    with open(os.path.join(test_pos_dir, filename)) as f:\n",
    "        text = f.readlines()[0]\n",
    "        df_test = df_test.append({'words': text, 'sentiment': 1},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "test_neg_dir = '../dataset/test/neg'\n",
    "for filename in os.listdir(test_neg_dir):\n",
    "    with open(os.path.join(test_neg_dir, filename)) as f:\n",
    "        text = f.readlines()[0]\n",
    "        df_test = df_test.append({'words': text, 'sentiment': 0},\n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    \n",
    "    def change(ch):\n",
    "        if ch in string.punctuation or ch.isdigit():\n",
    "            return \" \"\n",
    "        else:\n",
    "            return ch\n",
    "    \n",
    "    no_punct = \"\".join([change(ch) for ch in text])\n",
    "    return no_punct\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: remove_punct(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: tokenize(x.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(tokens):\n",
    "    text = [w for w in tokens if w not in stop_words]\n",
    "    return text\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: /remove_sw(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short(tokens):\n",
    "    text = [w for w in tokens if len(w)>2]\n",
    "    return text\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: remove_short(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to get root of tokens like stemming and lemmatizing. stemming is faster and lemmatizing is more precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizing(tokens):\n",
    "    text = [wn.lemmatize(w) for w in tokens]\n",
    "    return text\n",
    "\n",
    "# df_train['words'] = df_train['words'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_punct(text)\n",
    "    text = tokenize(text)\n",
    "    text = remove_sw(text)\n",
    "    text = remove_short(text)\n",
    "    text = lemmatizing(text)\n",
    "    return text\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text,\n",
    "                             lowercase=True,\n",
    "                             binary=True)\n",
    "X_train = count_vect.fit_transform(df_train['words'])\n",
    "y_train = df_train['sentiment'].to_numpy(dtype='int')\n",
    "\n",
    "X_test = count_vect.transform(df_test['words'])\n",
    "y_test = df_train['sentiment'].to_numpy(dtype='int')\n",
    "# print(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha=100)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "acc_tr = clf.score(X_train, y_train)\n",
    "acc_te = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"train accuracy: {}%\".format(acc_tr*100))\n",
    "print(\"test accuracy: {}%\".format(acc_te*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [10**x for x in range(-4, 4)]\n",
    "\n",
    "accs_tr = []\n",
    "accs_te = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    cls = MultinomialNB(alpha=alpha)\n",
    "    cls = cls.fit(X_train, y_train)\n",
    "    accs_tr.append(cls.score(X_train, y_train))\n",
    "    accs_te.append(cls.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(alphas, accs_tr)\n",
    "plt.plot(alphas, accs_te)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train accuracy', 'test accuracy'])\n",
    "plt.xscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
